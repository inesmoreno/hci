<u> Critiques with testing tasks, procedure, and prototype: </u> <br />
In general, many features that should have been tested were not included in the test, and features that were tested were implemented in a very limited way. Many important aspects of this tool that should have been tested were not tested. The testing procedures also left the evaluators and participants confused with the application. Specifically: 
<br />

<ol>
  <li>Presenter version of the application was not tested</li>
  <ul>
    <li>This severely undermines the utility of this testing as one of the two core audiences of your application are
      not assessed at all
    </li>
    <li>We were considering doing such testing for you, but we found the prototype to be broken for the presenter view
      and literally no testing materials were developed for the presenter version. We could not afford to create such
      testing materials for this evaluation.
    </li>
    <li><u>Modification:</u>create usability test materials for the presenter version.
    </li>
  </ul>

  <li>Core functionality of the application was not clear</li>
  <ul>
    <li>There was no concise or consistent definition of the core functionality of the application, including in the user documents. For example, new features of the application are discussed in different places. At the very end of the deliverable, the usability heuristics mentions “Once a visualization of an object is generated, everyone in the meeting should receive that image since others might have the same confusion or they just want to have a better view of it.” This is mentioned literally nowhere else in the deliverable, or user documents, or testing documents and adds more confusion to the purpose of the application.
    </li>
    <li>We were considering doing such testing for you, but we found the prototype to be broken for the presenter view
      and literally no testing materials were developed for the presenter version. We could not afford to create such
      testing materials for this evaluation.
    </li>
    <li><u>Modification:</u> State clearly the object of the application.
    </li>
  </ul>

  <li>Presenter version of the application was not tested</li>
  <ul>
    <li>This severely undermines the utility of this testing as one of the two core audiences of your application are
      not assessed at all
    </li>
    <li>We were considering doing such testing for you, but we found the prototype to be broken for the presenter view
      and literally no testing materials were developed for the presenter version. We could not afford to create such
      testing materials for this evaluation.
    </li>
    <li><u>Modification: </u>create usability test materials for the presenter version.
    </li>
  </ul>

   <li>Changes in design evolution were not tested</li>
  <ul>
    <li>Design evolution notes “Now, no pop-up messages for presenters to accept or decline a request. Alternatively, presenters can check a request list.” 
This should have been tested. Instead, testers were told to check the list
    </li>
    <li>Concern is, what if the presenter never notices the easyask queue and completely forgets about it? There were no visual indications of ppl entering the queue! 
This could be a serious design flaw that is not tested
    </li>
    <li><u>Modification:</u>Make targeted tasks and assessments for the changes in the design evolution.
    </li>
  </ul> 
    <li>No 3D object was used in the assessment</li>
  <ul>
    <li>This diminishes the value of the testing as zooming in on or manipulating a 3D object is very different than a 2d object!

    </li>
    <li>We understand it might not be feasible to truly embed a 3D object in a prototype. But at the very least an effort could be made to simulate rotating an 3rd object just using a handful of 2d images. Instead, cursor button wasn’t even hooked up to any functionality.
    </li>
    <li><u>Modification: </u>Implement the “move image using cursor” feature in the prototype. If you are aiming to use this feature with 3D models, give the illusion of a 3D model by having several 2D images of the same object from different perspectives to give a feel for this feature. .
    </li>
  </ul>

   <li>Usability evaluation was the same as the user tests</li>
  <ul>
    <li>There didn’t seem to be a test result sheet for the evaluators to use. The “usability evaluation” read much like the “post test questionnaire” asking for personal sentiments like “I feel like I was able to…”. It was not immediately clear who was supposed to fill out this questionnaire. We chose to ask the participant these questions as it made more sense, but seemed to overlap significantly with the post test questionnaire
    </li>
    <li><u>Modification: </u>make a data collection sheet for the evaluators
    </li>
  </ul> 
  
    <li>Test script and tasks largely do not align with usability goals or heuristics</li>
  <ul>
    <li>The tasks presented to the testers were very specific and do not provide an accurate assessment of the applications utility. Many of the tasks were too targeted like “click on the EASYASK icon” then “click on queue yourself”. This means test results are really testing a user’s ability to follow simple instructions instead of testing their ability to use your application more broadly.

    </li>
   
    <li><u>Modification: </u> change test task to be more general, such as: add yourself to the question queue, dequeue yourself from the question queue, manipulate the image to gain a better understanding of it and answer a question about the object</li>
  </ul>
</ol>